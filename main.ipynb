{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b854b07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando com o modelo: ViT_large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando bart: 100%|██████████| 78/78 [00:29<00:00,  2.68it/s]\n",
      "Processando homer: 100%|██████████| 61/61 [00:20<00:00,  2.96it/s]\n",
      "Processando lisa: 100%|██████████| 33/33 [00:10<00:00,  3.09it/s]\n",
      "Processando maggie: 100%|██████████| 30/30 [00:09<00:00,  3.09it/s]\n",
      "Processando marge: 100%|██████████| 24/24 [00:09<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos do modelo ViT_large salvos em: D://trabFinalIA\\result_final_ViT_large.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import ViTModel, ViTImageProcessor, AutoModel, CLIPImageProcessor\n",
    "import timm\n",
    "import open_clip\n",
    "from torchvision import transforms\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def load_models(model_name):\n",
    "    # Modelos baseados em ViT (transformers e auto model)\n",
    "    if model_name == 'ViT_huge':\n",
    "        model = ViTModel.from_pretrained('google/vit-huge-patch14-224-in21k')\n",
    "        feature_extractor = ViTImageProcessor.from_pretrained('google/vit-huge-patch14-224-in21k')\n",
    "    elif model_name == 'ViT_large':\n",
    "        model = ViTModel.from_pretrained('google/vit-large-patch16-224-in21k')\n",
    "        feature_extractor = ViTImageProcessor.from_pretrained('google/vit-large-patch16-224-in21k')\n",
    "    elif model_name == 'ViT_base':\n",
    "        model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        feature_extractor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "    elif model_name == 'ViT_small':\n",
    "        model = AutoModel.from_pretrained('WinKawaks/vit-small-patch16-224')\n",
    "        feature_extractor = ViTImageProcessor.from_pretrained('WinKawaks/vit-small-patch16-224')\n",
    "    elif model_name == 'VITAmin':\n",
    "        # Carrega o modelo ViTami\n",
    "        model = timm.create_model('vitamin_large_384', pretrained=True, num_classes=0)\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "        data_config = timm.data.resolve_model_data_config(model)\n",
    "        feature_extractor = timm.data.create_transform(**data_config, is_training=False)\n",
    "        \n",
    "        #model = AutoModel.from_pretrained('jienengchen/ViTamin-XL-384px', trust_remote_code=True)\n",
    "        #feature_extractor = CLIPImageProcessor.from_pretrained('jienengchen/ViTamin-XL-384px')\n",
    "    # Modelo openclip\n",
    "    elif model_name == 'openclip_vitg14':\n",
    "        model, _, feature_extractor = open_clip.create_model_and_transforms('ViT-g-14', pretrained='laion2b_s12b_b42k')\n",
    "    # Modelo mambaout (já existente)\n",
    "    elif model_name == 'mambaout':\n",
    "        model = timm.create_model('mambaout_base_plus_rw.sw_e150_r384_in12k_ft_in1k', pretrained=True, num_classes=0)\n",
    "        data_config = timm.data.resolve_model_data_config(model)\n",
    "        feature_extractor = timm.data.create_transform(**data_config, is_training=False)\n",
    "    # Modelo MahmoodLab/UNI (ViT-L/16 via DINOv2)\n",
    "    #elif model_name == 'UNI':\n",
    "        #from huggingface_hub import login\n",
    "        #from timm.data import resolve_data_config\n",
    "        #from timm.data.transforms_factory import create_transform\n",
    "        \n",
    "        # Faça login com HF_TOKEN antes de rodar\n",
    "        #login()\n",
    "        # Carrega backbone ViT-L/16 pré-treinado em Mass-100K (DINOv2)\n",
    "        #model = timm.create_model('hf-hub:MahmoodLab/uni',pretrained=True, init_values=1e-5,dynamic_img_size=True)\n",
    "        # Cria transform baseado na pré-configuração do modelo\n",
    "        #data_config = resolve_data_config(model.pretrained_cfg, model=model)\n",
    "        #feature_extractor = create_transform(**data_config)\n",
    "    \n",
    "    # Modelo prov-gigapath/prov-gigapath (tile encoder)\n",
    "    #elif model_name == 'prov_gigapath':\n",
    "        #import timm\n",
    "        #from torchvision import transforms\n",
    "        \n",
    "        # Tile encoder pré-treinado\n",
    "    #    model = timm.create_model(\n",
    "    #        \"hf_hub:prov-gigapath/prov-gigapath\",\n",
    "    #        pretrained=True\n",
    "    #    )\n",
    "        # Transforms recomendados: Resize→CenterCrop→ToTensor→Normalize\n",
    "    #    feature_extractor = transforms.Compose([\n",
    "    #        transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    #        transforms.CenterCrop(224),\n",
    "    #        transforms.ToTensor(),\n",
    "    #        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "    #                             std=(0.229, 0.224, 0.225))\n",
    "    #    ])   \n",
    "    # Novas arquiteturas do timm/huggingface\n",
    "    elif model_name in ['resnet18', 'squeezenet1_0', 'resnet50', 'resnet101', 'efficientnet_b0', \n",
    "                        'inception_resnet_v2', 'nasnetalarge', 'inception_v3', 'xception', 'darknet53', \n",
    "                        'vit_so400m_patch14_siglip_378.webli_ft_in1k', \n",
    "                        'mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k', 'mobilenetv4_hybrid_large.ix_e600_r384_in1k', \n",
    "                        'convnextv2_huge.fcmae_ft_in22k_in1k_384']:\n",
    "        # Certifique-se que os nomes abaixo correspondem aos identificadores disponíveis no timm\n",
    "        model = timm.create_model(model_name, pretrained=True, num_classes=0)\n",
    "        data_config = timm.data.resolve_model_data_config(model)\n",
    "        feature_extractor = timm.data.create_transform(**data_config, is_training=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo {model_name} não suportado.\")\n",
    "    \n",
    "    model.eval()\n",
    "    return model.to(device), feature_extractor\n",
    "\n",
    "\n",
    "def feature_extraction(image_path, model, feature_extractor, model_name):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Para modelos Transformers (ViTs e afins)\n",
    "    if model_name in ['ViT_huge', 'ViT_large', 'ViT_base', 'ViT_small']:\n",
    "        inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Aqui estamos usando a média dos embeddings da última camada\n",
    "        features = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    \n",
    "    # Para modelo openclip\n",
    "    elif 'openclip' in model_name:\n",
    "        image_tensor = feature_extractor(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            feats = model.encode_image(image_tensor)\n",
    "        features = feats.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Para modelos que possuem método forward_features (alguns modelos do timm)\n",
    "    elif hasattr(model, 'forward_features'):\n",
    "        input_tensor = feature_extractor(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            feats = model.forward_features(input_tensor)\n",
    "            feats = model.forward_head(feats, pre_logits=True)\n",
    "        features = feats.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Para modelos do timm (demais)\n",
    "    else:\n",
    "        input_tensor = feature_extractor(image)\n",
    "        if not torch.is_tensor(input_tensor):\n",
    "            raise ValueError(\"A transformação do timm não retornou um tensor.\")\n",
    "        if input_tensor.ndim == 3:\n",
    "            input_tensor = input_tensor.unsqueeze(0)\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        with torch.no_grad():\n",
    "            feats = model(input_tensor)\n",
    "        features = feats.squeeze().cpu().numpy()\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def features_to_df(folder_path, model, feature_extractor, model_name):\n",
    "    data = []\n",
    "    for subfolder in os.listdir(folder_path):\n",
    "        subfolder_path = os.path.join(folder_path, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            for image_file in tqdm(os.listdir(subfolder_path), desc=f\"Processando {subfolder}\"):\n",
    "                image_path = os.path.join(subfolder_path, image_file)\n",
    "                if image_file.lower().endswith(('png', 'jpg', 'jpeg', '.bmp')):\n",
    "                    try:\n",
    "                        feats = feature_extraction(image_path, model, feature_extractor, model_name)\n",
    "                        data.append([image_path, *feats])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Erro ao processar a imagem {image_path}: {e}\")\n",
    "    if data:\n",
    "        num_features = len(data[0]) - 1\n",
    "    else:\n",
    "        num_features = 0\n",
    "    columns = ['image_path'] + [f'feature_{i}' for i in range(num_features)]\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_dataframe_to_csv(df, save_path, file_name, model_name):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    full_path = os.path.join(save_path, file_name + '.csv')\n",
    "    df.to_csv(full_path, index=False)\n",
    "    print(f\"Arquivos do modelo {model_name} salvos em: {full_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_choices = [\n",
    "        'ViT_large', \n",
    "        #'ViT_huge', 'ViT_large', 'ViT_base', 'ViT_small', 'VITAmin', \n",
    "        #'mambaout', 'openclip_vitg14',\n",
    "       #\n",
    "        #'resnet18', 'squeezenet1_0', 'resnet50', 'resnet101', 'efficientnet_b0', \n",
    "        #'inception_resnet_v2', 'nasnetalarge', 'inception_v3', 'xception', 'darknet53', \n",
    "        #'vit_so400m_patch14_siglip_378.webli_ft_in1k', \n",
    "        #'mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k', \n",
    "        #'mobilenetv4_hybrid_large.ix_e600_r384_in1k', 'convnextv2_huge.fcmae_ft_in22k_in1k_384',\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    #fonte_folder = '/home/diego/Documents/posdoc_2025/database/FMD/images/'\n",
    "    #fonte_folder = '/home/diego/code/features/posdoc_2025/FMD/'\n",
    "    #salvar_folder = '/home/diego/code/features/posdoc_2025/feature_vector/FMD/'\n",
    "\n",
    "\n",
    "    fonte_folder = 'D:/trabFinalIA/simpsons/Train'\n",
    "    salvar_folder = 'D://trabFinalIA'\n",
    "        \n",
    "    #fonte_folder = 'home/diego/code/features/SOYPR/Fold1/'\n",
    "    #salvar_folder ='home/diego/code/features/posdoc_2025/python_code/experimentosSOYPR/'\n",
    "    \n",
    "\n",
    "\n",
    "    for model_choice in model_choices:\n",
    "        try:\n",
    "            print(f\"Processando com o modelo: {model_choice}\")\n",
    "            model, feat_ext = load_models(model_choice)\n",
    "            df = features_to_df(fonte_folder, model, feat_ext, model_choice)\n",
    "            file_name = f'result_final_{model_choice}'\n",
    "            save_dataframe_to_csv(df, salvar_folder, file_name, model_choice)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar o modelo {model_choice}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
